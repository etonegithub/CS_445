{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4rvp5urZrCk"
      },
      "source": [
        "$\\newcommand{\\xv}{\\mathbf{x}}\n",
        "\\newcommand{\\Xv}{\\mathbf{X}}\n",
        "\\newcommand{\\yv}{\\mathbf{y}}\n",
        "\\newcommand{\\zv}{\\mathbf{z}}\n",
        "\\newcommand{\\av}{\\mathbf{a}}\n",
        "\\newcommand{\\Wv}{\\mathbf{W}}\n",
        "\\newcommand{\\wv}{\\mathbf{w}}\n",
        "\\newcommand{\\tv}{\\mathbf{t}}\n",
        "\\newcommand{\\Tv}{\\mathbf{T}}\n",
        "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
        "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
        "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
        "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
        "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
        "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
        "\\newcommand{\\half}{\\frac{1}{2}}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRk7OuvPZrCm"
      },
      "source": [
        "# Trackmania Reinforcement Learning Project Proposal for CS445"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S04DuFLUZrCn"
      },
      "source": [
        "Kushal Reddy Alimineti, Evan Tone, Henry Gates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd_iSo6aZrCn"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRC8FoLpZrCn"
      },
      "source": [
        "In this project, we aim to answer the following research questions:\n",
        "\n",
        "  * Can a reinforcement learning (RL) agent trained on maze-like tracks in Trackmania learn to generalize the concept of navigating a racetrack?\n",
        "\n",
        "\n",
        "  * Will the trained model be able to complete a novel maze-like track it has not seen during training?\n",
        "\n",
        "\n",
        "  * What RL algorithms are most effective for learning this task in terms of convergence speed and final performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hypothesis"
      ],
      "metadata": {
        "id": "-bAATQsOb_Zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * We hypothesize that an RL agent trained on several maze-like tracks will learn the core idea of moving forward, staying on track, and avoiding crashes.\n",
        "\n",
        "\n",
        "  * We expect the model to partially succeed on unseen tracks, showing some level of generalization but not perfect performance due to overfitting to training tracks or limited environment complexity.\n"
      ],
      "metadata": {
        "id": "ytFl9-9Bb4mq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjuPK-hbZrCn"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akBSSJAZZrCo"
      },
      "source": [
        "Explain why you wish to do the proposed project and the steps you will take to complete the project. Describe the methods you will use.  What are the sources of code or data?  Will you define new algorithms and/or implementations, or download ones from an on-line source?\n",
        "\n",
        "You may work on your own, or form a team of up to four other students in this class. In this proposal, define how the work on the project will be divided among the team members.  It is not sufficient to just state that all team members will be working together.  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------"
      ],
      "metadata": {
        "id": "3Tz4y06cc7Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We chose this project to explore reinforcement learning in a dynamic, interactive environment that resembles real-world navigation challenges. Trackmania provides a useful simulated environment with customizable tracks, making it suitable for RL research.\n",
        "###Steps and Methods:\n",
        "####Data and Code Sources:\n",
        "  * Game telemetry data from Trackmania\n",
        "\n",
        "\n",
        "  * RL code base: stable-baselines3, custom wrappers, and potential GitHub libraries (TMInterface for setting up the simulation)\n",
        "\n",
        "\n",
        "  * No new algorithm will be defined from scratch, but significant engineering effort will be used to adapt the RL agent to Trackmania\n",
        "  \n",
        "####Environment Setup:\n",
        "  * Create several custom maze-like tracks in Trackmania using the in-game track editor.\n",
        "\n",
        "\n",
        "  * Connect the Trackmania environment to our RL model using available APIs (e.g., TMInterface or Python bindings with the OpenPlanet plugin).\n",
        "\n",
        "\n",
        "####Model and Training:\n",
        "\n",
        "\n",
        "  * Implement a reinforcement learning algorithm (starting with Deep Q-Learning or PPO).\n",
        "\n",
        "\n",
        "  * Use Python with stable-baselines3 or custom RL implementations.\n",
        "\n",
        "\n",
        "  * Train the model over multiple episodes across different training tracks.\n",
        "\n",
        "\n",
        "####Testing and Analysis:\n",
        "\n",
        "\n",
        "  * Test the trained model on a new, unseen track.\n",
        "\n",
        "\n",
        "  * Record performance metrics: success rate, average time to finish, number of crashes, etc.\n",
        "\n",
        "\n",
        "  * Visualize agent behavior and analyze what strategies were learned.\n"
      ],
      "metadata": {
        "id": "PfEOnPW3c6Dd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KN2IXDEZrCo"
      },
      "source": [
        "## Possible Results\n",
        "\n",
        "Speculate on possible answers to the questions you provide in the Introduction.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "  * The result would be a simulation of the car that reaches the end of a unseen maze-like track within a resonable amount of time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUuOVWKZZrCo"
      },
      "source": [
        "## Timeline\n",
        "--------------------------------------------------------------------------\n",
        "* April 7 - 14 -> Get Environment created and all necessary APIS to train\n",
        "* April 14 - 24 -> Implement RL algorithim and begin training\n",
        "* April 24 - April 29 -> Finish training and optimizations, Begin Tests\n",
        "* April 29 - May 8th -> Final tests, Complete write up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------"
      ],
      "metadata": {
        "id": "XKF4CUs3dlv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Team Roles and Work Division\n",
        "####We are a team of 3 members. We will be working together on each of the steps of this project, so that no member is waiting on another to finish before they can start working:\n",
        "  * Step 1: Track creation, environment integration with the RL agent, data collection.\n",
        "  * Step 2: Reinforcement learning implementation, agent model development, testing the trained model.\n",
        "  * Step 3: Analysis of results, metrics tracking, and visualization, contributes to model evaluation and testing.\n"
      ],
      "metadata": {
        "id": "yPi506-Yd_AG"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}